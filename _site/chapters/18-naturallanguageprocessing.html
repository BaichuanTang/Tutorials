<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Natural Language Processing - Textbooks with Jupyter</title>
<meta name="description" content="Natural Language Processing">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Textbooks with Jupyter">
<meta property="og:title" content="Natural Language Processing">
<meta property="og:url" content="http://localhost:4000/textbooks-with-jupyter/chapters/18-NaturalLanguageProcessing">


  <meta property="og:description" content="Natural Language Processing">







  <meta property="article:published_time" content="2018-09-21T12:22:31-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/textbooks-with-jupyter/chapters/18-NaturalLanguageProcessing">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Chris Holdgraf",
      "url": "http://localhost:4000/textbooks-with-jupyter",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/textbooks-with-jupyter/feed.xml" type="application/atom+xml" rel="alternate" title="Textbooks with Jupyter Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/textbooks-with-jupyter/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->


<!-- end custom head snippets -->

    <link rel="stylesheet" href="/textbooks-with-jupyter/assets/css/notebook-markdown.css">
    <link rel="stylesheet" href="/textbooks-with-jupyter/assets/css/custom.css">
    <link rel="shortcut icon" type="image/png" href="/textbooks-with-jupyter/favicon.png">
    <script src="https://cdn.jsdelivr.net/npm/clipboard@1/dist/clipboard.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js"></script>
  </head>

  <body class="layout--textbook">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

    <div class="initial-content">
      



<div id="main" class="textbook" role="main">
  <div id="textbook_wrapper">
    
  <div class="sidebar sticky textbook">
  
  
    <img src="/textbooks-with-jupyter/images/logo/logo.png" class="textbook_logo" />
    

    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/"><span class="nav__sub-title">Home</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/00-Introduction"><span class="nav__sub-title">00-introduction</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/01-JupyterNotebooks"><span class="nav__sub-title">01-jupyternotebooks</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/02-DataAnalysis"><span class="nav__sub-title">02-dataanalysis</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/03-Python"><span class="nav__sub-title">03-python</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/04-DataSciencePython"><span class="nav__sub-title">04-datasciencepython</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/05-DataGathering"><span class="nav__sub-title">05-datagathering</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/06-DataWrangling"><span class="nav__sub-title">06-datawrangling</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/07-DataCleaning"><span class="nav__sub-title">07-datacleaning</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/08-DataPrivacy&Anonymization"><span class="nav__sub-title">08-dataprivacy&anonymization</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/09-DataVisualization"><span class="nav__sub-title">09-datavisualization</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/10-Distributions"><span class="nav__sub-title">10-distributions</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/11-TestingDistributions"><span class="nav__sub-title">11-testingdistributions</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/13-OrdinaryLeastSquares"><span class="nav__sub-title">13-ordinaryleastsquares</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/14-LinearModels"><span class="nav__sub-title">14-linearmodels</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/15-Clustering"><span class="nav__sub-title">15-clustering</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/16-DimensionalityReduction"><span class="nav__sub-title">16-dimensionalityreduction</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/17-Classification"><span class="nav__sub-title">17-classification</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/18-NaturalLanguageProcessing"><span class="nav__sub-title">18-naturallanguageprocessing</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/A1-PythonPackages"><span class="nav__sub-title">A1-pythonpackages</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/A2-Git"><span class="nav__sub-title">A2-git</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/textbooks-with-jupyter/chapters/README"><span class="nav__sub-title">Readme</span></a>
        

        
      </li>
    
  </ul>
</nav>

    

  
  </div>


    <article class="page textbook" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="headline" content="Natural Language Processing">
      <meta itemprop="description" content="Natural Language Processing">
      <meta itemprop="datePublished" content="September 21, 2018">
      

      <div class="page__inner-wrap">
        
          <header>
            <h1 id="page-title" class="page__title" itemprop="headline">Natural Language Processing
</h1>
          </header>
        

        <section class="page__content" itemprop="text">
          
            

<!-- TOC will only show up if it has at least one item -->


  <aside class="sidebar__right">
    <nav class="toc">
      <header><h4 class="nav__title"><i class="fas fa-list-ul"></i>   On this page</h4></header>
      <ul class="toc__menu">
  <li><a href="#ntlk-natural-language-tool-kit">NTLK: Natural Language Tool Kit</a></li>
  <li><a href="#tokenisation">Tokenisation</a></li>
  <li><a href="#part-of-speech-pos-tagging">Part-of-speech (POS) Tagging</a></li>
  <li><a href="#named-entity-recognition-ner">Named Entity Recognition (NER)</a></li>
  <li><a href="#stop-words">Stop words</a></li>
</ul>
    </nav>
  </aside>


          
          <!-- INTERACT LINKS -->

    
    
    <a class="interact-button" href="https://mybinder.org/v2/gh/choldgraf/textbooks-with-jupyter/master?filepath=notebooks%2F18-NaturalLanguageProcessing.ipynb">Interact</a>


          
<div class="alert alert-success">
Natural Language Processing (NLP) is the approach of analyzing text data, with computers.
</div>

<div class="alert alert-info">
Natural Language Processing on 
&lt;a href=https://en.wikipedia.org/wiki/Natural-language_processing&gt;wikipedia&lt;/a&gt;.
</div>

<h2 id="ntlk-natural-language-tool-kit">NTLK: Natural Language Tool Kit</h2>

<div class="alert alert-success">
NLTK is the main Python module for text-analysis. 
</div>

<div class="alert alert-info">
The NLTK organization website is 
&lt;a href=http://www.nltk.org/&gt;here&lt;/a&gt;
and they have a whole book of tutorials 
&lt;a href=http://www.nltk.org/book/&gt;here&lt;/a&gt;.
</div>

<h4 id="nltk">NLTK</h4>

<p>NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Import NLTK</span>
<span class="kn">import</span> <span class="nn">nltk</span>
</code></pre></div></div>

<p>In this notebook, we will walk through some basic text-analysis using some useful functions from the NLTK package.</p>

<p>To work with text-data, you often need corpora - text datasets to compare to. NLTK has many such datasets available, but doesn’t install them by default (as the full set of them would be quite large). Below we will download some of these datasets.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># If you hit an error downloading things in the cell below, come back to this cell, uncomment it, and run this code.</span>
<span class="c">#   This code gives python permission to write to your disk (if it doesn't already have persmission to do so).</span>
<span class="kn">import</span> <span class="nn">ssl</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">_create_unverified_https_context</span> <span class="o">=</span> <span class="n">ssl</span><span class="o">.</span><span class="n">_create_unverified_context</span>
<span class="k">except</span> <span class="nb">AttributeError</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">ssl</span><span class="o">.</span><span class="n">_create_default_https_context</span> <span class="o">=</span> <span class="n">_create_unverified_https_context</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Download some useful data files from NLTK</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s">'punkt'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s">'stopwords'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s">'averaged_perceptron_tagger'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s">'maxent_ne_chunker'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s">'words'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s">'treebank'</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[nltk_data] Downloading package punkt to /Users/tom/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /Users/tom/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /Users/tom/nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
[nltk_data] Downloading package maxent_ne_chunker to
[nltk_data]     /Users/tom/nltk_data...
[nltk_data]   Package maxent_ne_chunker is already up-to-date!
[nltk_data] Downloading package words to /Users/tom/nltk_data...
[nltk_data]   Package words is already up-to-date!
[nltk_data] Downloading package treebank to /Users/tom/nltk_data...
[nltk_data]   Package treebank is already up-to-date!

</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set some test sentences of data to play with</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s">"UC San Diego is a great place to study cognitive science."</span>
</code></pre></div></div>

<h2 id="tokenisation">Tokenisation</h2>

<div class="alert alert-success">
Tokenization is the process of splitting text data into 'tokens', which are meaningful pieces of data.
</div>

<div class="alert alert-info">
More information on tokenization
&lt;a href=https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html&gt;here&lt;/a&gt;.
</div>

<p>Tokenization can be done at different levels - you can, for example tokenize text into sentences, and/or into words.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Tokenize our sentence, at the word level</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check out the word-tokenized data</span>
<span class="k">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['UC', 'San', 'Diego', 'is', 'a', 'great', 'place', 'to', 'study', 'cognitive', 'science', '.']

</code></pre></div></div>

<h2 id="part-of-speech-pos-tagging">Part-of-speech (POS) Tagging</h2>

<div class="alert alert-success">
Part-of-Speech tagging is the process of labelling words with respect to their 'types' and relationships to other words.
</div>

<div class="alert alert-info">
Part-of-speech tagging on 
&lt;a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging&gt;wikipedia&lt;/a&gt;.
</div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Apply part-of-speech tagging to our sentence</span>
<span class="n">tags</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check the POS tags for our data</span>
<span class="k">print</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('UC', 'NNP'), ('San', 'NNP'), ('Diego', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('place', 'NN'), ('to', 'TO'), ('study', 'VB'), ('cognitive', 'JJ'), ('science', 'NN'), ('.', '.')]

</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check the documentation that describes what all of the abbreviations mean</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">upenn_tagset</span><span class="p">()</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$: dollar
    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$
'': closing quotation mark
    ' ''
(: opening parenthesis
    ( [ {
): closing parenthesis
    ) ] }
,: comma
    ,
--: dash
    --
.: sentence terminator
    . ! ?
:: colon or ellipsis
    : ; ...
CC: conjunction, coordinating
    &amp; 'n and both but either et for less minus neither nor or plus so
    therefore times v. versus vs. whether yet
CD: numeral, cardinal
    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-
    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025
    fifteen 271,124 dozen quintillion DM2,000 ...
DT: determiner
    all an another any both del each either every half la many much nary
    neither no some such that the them these this those
EX: existential there
    there
FW: foreign word
    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous
    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte
    terram fiche oui corporis ...
IN: preposition or conjunction, subordinating
    astride among uppon whether out inside pro despite on by throughout
    below within for towards near behind atop around if like until below
    next into if beside ...
JJ: adjective or numeral, ordinal
    third ill-mannered pre-war regrettable oiled calamitous first separable
    ectoplasmic battery-powered participatory fourth still-to-be-named
    multilingual multi-disciplinary ...
JJR: adjective, comparative
    bleaker braver breezier briefer brighter brisker broader bumper busier
    calmer cheaper choosier cleaner clearer closer colder commoner costlier
    cozier creamier crunchier cuter ...
JJS: adjective, superlative
    calmest cheapest choicest classiest cleanest clearest closest commonest
    corniest costliest crassest creepiest crudest cutest darkest deadliest
    dearest deepest densest dinkiest ...
LS: list item marker
    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005
    SP-44007 Second Third Three Two * a b c d first five four one six three
    two
MD: modal auxiliary
    can cannot could couldn't dare may might must need ought shall should
    shouldn't will would
NN: noun, common, singular or mass
    common-carrier cabbage knuckle-duster Casino afghan shed thermostat
    investment slide humour falloff slick wind hyena override subhumanity
    machinist ...
NNP: noun, proper, singular
    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos
    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA
    Shannon A.K.C. Meltex Liverpool ...
NNPS: noun, proper, plural
    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists
    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques
    Apache Apaches Apocrypha ...
NNS: noun, common, plural
    undergraduates scotches bric-a-brac products bodyguards facets coasts
    divestitures storehouses designs clubs fragrances averages
    subjectivists apprehensions muses factory-jobs ...
PDT: pre-determiner
    all both half many quite such sure this
POS: genitive marker
    ' 's
PRP: pronoun, personal
    hers herself him himself hisself it itself me myself one oneself ours
    ourselves ownself self she thee theirs them themselves they thou thy us
PRP$: pronoun, possessive
    her his mine my our ours their thy your
RB: adverb
    occasionally unabatingly maddeningly adventurously professedly
    stirringly prominently technologically magisterially predominately
    swiftly fiscally pitilessly ...
RBR: adverb, comparative
    further gloomier grander graver greater grimmer harder harsher
    healthier heavier higher however larger later leaner lengthier less-
    perfectly lesser lonelier longer louder lower more ...
RBS: adverb, superlative
    best biggest bluntest earliest farthest first furthest hardest
    heartiest highest largest least less most nearest second tightest worst
RP: particle
    aboard about across along apart around aside at away back before behind
    by crop down ever fast for forth from go high i.e. in into just later
    low more off on open out over per pie raising start teeth that through
    under unto up up-pp upon whole with you
SYM: symbol
    % &amp; ' '' ''. ) ). * + ,. &lt; = &gt; @ A[fj] U.S U.S.S.R * ** ***
TO: "to" as preposition or infinitive marker
    to
UH: interjection
    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen
    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly
    man baby diddle hush sonuvabitch ...
VB: verb, base form
    ask assemble assess assign assume atone attention avoid bake balkanize
    bank begin behold believe bend benefit bevel beware bless boil bomb
    boost brace break bring broil brush build ...
VBD: verb, past tense
    dipped pleaded swiped regummed soaked tidied convened halted registered
    cushioned exacted snubbed strode aimed adopted belied figgered
    speculated wore appreciated contemplated ...
VBG: verb, present participle or gerund
    telegraphing stirring focusing angering judging stalling lactating
    hankerin' alleging veering capping approaching traveling besieging
    encrypting interrupting erasing wincing ...
VBN: verb, past participle
    multihulled dilapidated aerosolized chaired languished panelized used
    experimented flourished imitated reunifed factored condensed sheared
    unsettled primed dubbed desired ...
VBP: verb, present tense, not 3rd person singular
    predominate wrap resort sue twist spill cure lengthen brush terminate
    appear tend stray glisten obtain comprise detest tease attract
    emphasize mold postpone sever return wag ...
VBZ: verb, present tense, 3rd person singular
    bases reconstructs marks mixes displeases seals carps weaves snatches
    slumps stretches authorizes smolders pictures emerges stockpiles
    seduces fizzes uses bolsters slaps speaks pleads ...
WDT: WH-determiner
    that what whatever which whichever
WP: WH-pronoun
    that what whatever whatsoever which who whom whosoever
WP$: WH-pronoun, possessive
    whose
WRB: Wh-adverb
    how however whence whenever where whereby whereever wherein whereof why
``: opening quotation mark
    ` ``

</code></pre></div></div>

<h2 id="named-entity-recognition-ner">Named Entity Recognition (NER)</h2>

<div class="alert alert-success">
Named entity recognition seeks to label words with the kinds of entities that they relate to.
</div>

<div class="alert alert-info">
Named entity recognition on 
&lt;a href=https://en.wikipedia.org/wiki/Named-entity_recognition&gt;wikipedia&lt;/a&gt;.
</div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Apply named entity recognition to our POS tags</span>
<span class="n">entities</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">chunk</span><span class="o">.</span><span class="n">ne_chunk</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check out the named entities</span>
<span class="k">print</span><span class="p">(</span><span class="n">entities</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(S
  UC/NNP
  (PERSON San/NNP Diego/NNP)
  is/VBZ
  a/DT
  great/JJ
  place/NN
  to/TO
  study/VB
  cognitive/JJ
  science/NN
  ./.)

</code></pre></div></div>

<h2 id="stop-words">Stop words</h2>

<div class="alert alert-success">
'Stop words' are the most common words of a language, that we often want to filter out before text analysis. 
</div>

<div class="alert alert-info">
Stop words on 
&lt;a href=https://en.wikipedia.org/wiki/Stop_words&gt;wikipedia&lt;/a&gt;.
</div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check out the corpus of stop words in English</span>
<span class="k">print</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">))</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

</code></pre></div></div>

<h1 id="text-encoding">Text Encoding</h1>

<p>One of the key components of NLP, is deciding how to encode the text data.</p>

<p>Common encodings are:</p>
<ul>
  <li>Bag of Words (BoW)
    <ul>
      <li>Text is encoded as a collection of words &amp; frequencies</li>
    </ul>
  </li>
  <li>Term Frequency / Inverse Document Frequency (TF/IDF)
    <ul>
      <li>TF/IDF is a weighting that stores words with relation to their commonality across a corpus.</li>
    </ul>
  </li>
</ul>

<p>We will walk through an example of encoding text as BoW and TF-IDF.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Imports</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c"># Standard Python has some useful string tools</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="c"># Collections is a part of standard Python, with some useful data objects</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c"># Scikit-learn has some useful NLP tools, such as a TFIDF vectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
</code></pre></div></div>

<p>The data we will be looking at is a small subset of the BookCorpus dataset. The original dataset can be found here: http://yknzhu.wixsite.com/mbweb.</p>

<p>The original dataset was collected from more than 11,000 books, and has already been tokenised at both the sentence and word level. The small subset provided and used here contains the first 10,000 sentences.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Load the data</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'files/book10k.txt'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">sents</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check out the data - print out the first and last sentences, as examples</span>
<span class="k">print</span><span class="p">(</span><span class="n">sents</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">sents</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>the half-ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved .

alejo was sure the fact that he was nervously repeating mass along with five wrinkly , age-encrusted spanish women meant that stalin was rethinking whether he was going to pay the price .


</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Preprocessing: Strip all extra whitespace from the sentences</span>
<span class="n">sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">sent</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sents</span><span class="p">]</span>
</code></pre></div></div>

<p>We first take a look at the word frequency in the document, and print out top 10 most frequently appeared words with their frequencies.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Tokenize all the sentences into words</span>
<span class="c">#  This collects all the word tokens together into one big list</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">sents</span><span class="p">:</span>
    <span class="n">tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check out how many words are in the data</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Number of words in the data: </span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Number of unique words: </span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">)))</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of words in the data: 	 140060
Number of unique words: 	 8221

</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use the 'counter' object to count how many times each word appears</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check out the counts object</span>
<span class="c">#  This is basically a 'bag-of-words' representation of this corpus</span>
<span class="c">#  We have lost word order and grammar - it's just a collection of words</span>
<span class="c">#  What we do have is a list of all the words present, and how often they appear</span>
<span class="n">counts</span>
</code></pre></div></div>

<p>One thing you might notice if you scroll through the word list above is that it still contains punctuation. Let’s remove those.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The 'string' module (standard library) has a useful list of punctuation</span>
<span class="k">print</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~

</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Drop all punction markers from the counts object</span>
<span class="k">for</span> <span class="n">punc</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">punc</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">:</span>
        <span class="n">counts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">punc</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get the top 10 most frequent words</span>
<span class="n">top10</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Extract the top words, and counts</span>
<span class="n">top10_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">it</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">top10</span><span class="p">]</span>
<span class="n">top10_counts</span> <span class="o">=</span> <span class="p">[</span><span class="n">it</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">top10</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plot a barplot of the most frequent words in the text</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">top10_words</span><span class="p">,</span> <span class="n">top10_counts</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Term Frequency'</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Frequency'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../images/./_chapters/18-NaturalLanguageProcessing_41_0.png" alt="png" /></p>

<p>As we can see, ‘the’, ‘was’, ‘a’, etc. appear a lot in the document.</p>

<p>These frequently appearing words aren’t really that useful to figure out what these documents are about, or as a way to use and understand this text data.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Drop all stop words</span>
<span class="k">for</span> <span class="n">stop</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">stop</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">:</span>
        <span class="n">counts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">stop</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get the top 20 most frequent words, of the stopword-removed data</span>
<span class="n">top20</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plot a barplot of the most frequent words in the text</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">([</span><span class="n">it</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">top20</span><span class="p">],</span> <span class="p">[</span><span class="n">it</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">top20</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Term Frequency'</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Frequency'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../images/./_chapters/18-NaturalLanguageProcessing_45_0.png" alt="png" /></p>

<p>This looks potentially more relevant / useful. We could continue exploring this BoW model, but let’s instead pivot now, and explore using TFIDF.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Initialize a TFIDF object</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s">'word'</span><span class="p">,</span>
                        <span class="n">sublinear_tf</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">max_features</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
                        <span class="n">tokenizer</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Apply the TFIDF transformation to our data</span>
<span class="c">#  Note that this takes the sentences, and tokenizes them, then applies TFIDF</span>
<span class="n">tfidf_books</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">sents</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</code></pre></div></div>

<p>The TfidfVectorizer will calculate the inverse document frequency (IDF) for each word.</p>

<p>The TFIDF is then calculated as the TF * IDF, working to down-weight frequently appearing words. This TFIDF is stored in ‘tfidf_books’ variable, which is a n_documents x n_words matrix that encodes the documents in a TFIDF representation.</p>

<p>Let’s first plot out the IDF for each of the top 10 most frequently appeared words (from the first analysis).</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get the IDF weights for the top 10 most common words</span>
<span class="n">IDF_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">tfidf</span><span class="o">.</span><span class="n">idf_</span><span class="p">[</span><span class="n">tfidf</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="n">token</span><span class="p">]]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">top10_words</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Plot the IDF scores for the very common words</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">top10_words</span><span class="p">,</span> <span class="n">IDF_weights</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Inverse Document Frequency'</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'IDF Score'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../images/./_chapters/18-NaturalLanguageProcessing_51_0.png" alt="png" /></p>

<p>We compare the plot with the following plot that shows the words with top 10 highest IDF.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get the words with the highest IDF score</span>
<span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">tfidf</span><span class="o">.</span><span class="n">idf_</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
<span class="n">top_IDF_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">tfidf</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)[</span><span class="n">ind</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">inds</span><span class="p">]</span>
<span class="n">top_IDF_scores</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">idf_</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get the words with the highest IDF score</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">top_IDF_tokens</span><span class="p">,</span> <span class="n">top_IDF_scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Inverse Document Frequency'</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'IDF Score'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../images/./_chapters/18-NaturalLanguageProcessing_54_0.png" alt="png" /></p>

<p>As we can see, the frequently appearing words in the document get very low IDF scores, as compared to much rarer words.</p>

<p>After TF-IDF, we successfully down-weight the frequently appearing words in the document. This allows us to represent a document by the words that are most unique to it, which can be a more useful way to represent text data.</p>

          
        </section>

        <footer class="page__meta">
          
          


        </footer>

        

        
  <nav class="pagination">
    
      <a href="/textbooks-with-jupyter/chapters/17-Classification" class="pagination--pager" title="17-classification
">Previous</a>
    
    
      <a href="/textbooks-with-jupyter/chapters/A1-PythonPackages" class="pagination--pager" title="A1-pythonpackages
">Next</a>
    
  </nav>


      </div>

      
    </article>
  </div>
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    

    
  <script src="/textbooks-with-jupyter/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>




<script src="/textbooks-with-jupyter/assets/js/lunr/lunr.min.js"></script>
<script src="/textbooks-with-jupyter/assets/js/lunr/lunr-store.js"></script>
<script src="/textbooks-with-jupyter/assets/js/lunr/lunr-en.js"></script>




    <!-- Custom scripts to load after site JS is loaded -->

    <!-- Custom HTML used for the textbooks -->
<!-- Configure, then load MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      processEnvironments: true
    }
  };
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full,Safe" type="text/javascript"></script>


<script type="text/javascript">
// --- To auto-embed hub URLs in interact links if given in a RESTful fashion ---
function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return jQuery.param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = $("a").each(function() {
    var href = this.href;
    // If the link is an internal link...
    if (href.search("http://localhost:4000") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['hub'] = hub;
      } else {
        // Create the REST params
        params = {'hub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + jQuery.param(params);
      this.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}

  // Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    hubUrl = rest['hub'];
    if (hubUrl !== undefined) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);
      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      link = $("a.interact-button")[0];
      if (link !== undefined) {
          // Update the interact link URL
          var href = link.getAttribute('href');
          if ('binder' == 'binder') {
            // If binder links exist, we need to re-work them for jupyterhub
            first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
            href = first + '?' + binder2Jupyterhub(href);
          } else {
            // If JupyterHub links, we only need to replace the hub url
            href = href.replace("https://mybinder.org", hubUrl);
          }
          link.setAttribute('href', decodeURIComponent(href));

          // Add text after interact link saying where we're launching
          hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
          $("a.interact-button").after($('<div class="interact-context">on ' + hubUrlNoHttp + '</div>'));

      }
      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

// --- Highlight the part of sidebar for current page ---

// helper to replace trailing slash
function replaceSlash(string)
{
    return string.replace(/\/$/, "");
}

// Add a class to the current page in the sidebar
function highlightSidebarCurrentPage()
{
  var currentpage = location.href;
  var links = $('.sidebar .nav__items a');
  var ii = 0;
  for(ii; ii < links.length; ii++) {
    var link = links[ii];
    if(replaceSlash(link.href) == replaceSlash(currentpage)) {
      // Add CSS for styling
      link.classList.add("current");
      // Scroll to this element
      $('div.sidebar').scrollTop(link.offsetTop - 300);
    }
  }
}

// --- Set up copy/paste for code blocks ---
function addCopyButtonToCode(){
  // get all <code> elements
  var allCodeBlocksElements = $( "div.input_area code, div.highlighter-rouge code" );

  allCodeBlocksElements.each(function(ii) {
   	// add different id for each code block

  	// target
    var currentId = "codeblock" + (ii + 1);
    $(this).attr('id', currentId);

    //trigger
    var clipButton = '<button class="btn copybtn" data-clipboard-target="#' + currentId + '"><img src="https://clipboardjs.com/assets/images/clippy.svg" width="13" alt="Copy to clipboard"></button>';
       $(this).after(clipButton);
    });

    new Clipboard('.btn');
}

// Run scripts when page is loaded
$(document).ready(function () {
  // Add anchors to H1 etc links
  anchors.add();
  // Highlight current page in sidebar
  highlightSidebarCurrentPage();
  // Add copy button to code blocks
  addCopyButtonToCode();
  // Update the Interact link if a REST param given
  updateInteractLink();
});
</script>

  </body>
</html>
